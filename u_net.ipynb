{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "class TrainDataset(data.Dataset):\n",
    "    def __init__(self, root=''):\n",
    "        super(TrainDataset, self).__init__()\n",
    "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "        self.img_files.sort()\n",
    "        self.mask_files = []\n",
    "        for img_path in self.img_files:\n",
    "            basename = os.path.basename(img_path)\n",
    "            self.mask_files.append(os.path.join(root,'mask',basename[:-4]+'_mask.png'))\n",
    "        # get the path of these images  \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            mask_path = self.mask_files[index]\n",
    "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            label = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "            return torch.from_numpy(data).float(), torch.from_numpy(label).float(), img_path\n",
    "            #change it from numpy to tesnor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "        # how many train images\n",
    "\n",
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, root=''):\n",
    "        super(TestDataset, self).__init__()\n",
    "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "        self.img_files.sort()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            return torch.from_numpy(data).float(), img_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dice_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        if weight is not None:\n",
    "            weight = torch.Tensor(weight)\n",
    "            self.weight = weight / torch.sum(weight) # Normalized weight\n",
    "        self.smooth = 1e-5\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        N, C = predict.size()[:2]\n",
    "        predict = predict.view(N, C, -1) \n",
    "        target = target.view(N, 1, -1) \n",
    "\n",
    "        predict = F.softmax(predict, dim=1) \n",
    "        \n",
    "        target_onehot = torch.zeros(predict.size())  \n",
    "        target_onehot.scatter_(1, target, 1)  \n",
    "\n",
    "        intersection = torch.sum(predict * target_onehot, dim=2) \n",
    "        union = torch.sum(predict.pow(2), dim=2) + torch.sum(target_onehot, dim=2) \n",
    "        \n",
    "        dice_coef = (2 * intersection + self.smooth) / (union + self.smooth)  \n",
    "\n",
    "        if hasattr(self, 'weight'):\n",
    "            if self.weight.type() != predict.type():\n",
    "                self.weight = self.weight.type_as(predict)\n",
    "                dice_coef = dice_coef * self.weight * C  \n",
    "        dice_loss = 1 - torch.mean(dice_coef)  \n",
    "\n",
    "        return dice_loss\n",
    "    \n",
    "# criterion = DiceLoss(weight=[1, 1])\n",
    "# loss = criterion(y_predict, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#focal loss\n",
    "\n",
    "from typing import Optional, Sequence\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 alpha: Optional[Tensor] = None,\n",
    "                 gamma: float = 0.,\n",
    "                 reduction: str = 'mean',\n",
    "                 ignore_index: int = -100):\n",
    "\n",
    "        if reduction not in ('mean', 'sum', 'none'):\n",
    "            raise ValueError(\n",
    "                'Reduction must be one of: \"mean\", \"sum\", \"none\".')\n",
    "\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.nll_loss = nn.NLLLoss(\n",
    "            weight=alpha, reduction='none', ignore_index=ignore_index)\n",
    "\n",
    "    def __repr__(self):\n",
    "        arg_keys = ['alpha', 'gamma', 'ignore_index', 'reduction']\n",
    "        arg_vals = [self.__dict__[k] for k in arg_keys]\n",
    "        arg_strs = [f'{k}={v}' for k, v in zip(arg_keys, arg_vals)]\n",
    "        arg_str = ', '.join(arg_strs)\n",
    "        return f'{type(self).__name__}({arg_str})'\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        if x.ndim > 2:\n",
    "     \n",
    "            c = x.shape[1]\n",
    "            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n",
    "    \n",
    "            y = y.view(-1)\n",
    "\n",
    "        unignored_mask = y != self.ignore_index\n",
    "        y = y[unignored_mask]\n",
    "        if len(y) == 0:\n",
    "            return 0.\n",
    "        x = x[unignored_mask]\n",
    "\n",
    "\n",
    "        log_p = F.log_softmax(x, dim=-1)\n",
    "        ce = self.nll_loss(log_p, y)\n",
    "\n",
    "        all_rows = torch.arange(len(x))\n",
    "        log_pt = log_p[all_rows, y]\n",
    "\n",
    "        pt = log_pt.exp()\n",
    "        focal_term = (1 - pt)**self.gamma\n",
    "\n",
    "        loss = focal_term * ce\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def focal_loss(alpha: Optional[Sequence] = None,\n",
    "               gamma: float = 0.,\n",
    "               reduction: str = 'mean',\n",
    "               ignore_index: int = -100,\n",
    "               device='cpu',\n",
    "               dtype=torch.float32) -> FocalLoss:\n",
    "\n",
    "    if alpha is not None:\n",
    "        if not isinstance(alpha, Tensor):\n",
    "            alpha = torch.tensor(alpha)\n",
    "        alpha = alpha.to(device=device, dtype=dtype)\n",
    "\n",
    "    fl = FocalLoss(\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "        reduction=reduction,\n",
    "        ignore_index=ignore_index)\n",
    "    return fl\n",
    "\n",
    "# Loss = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "            \n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "\n",
    "        self.up1 = Up(1024, 256)\n",
    "        self.up2 = Up(512, 128)\n",
    "        self.up3 = Up(256, 64)\n",
    "        self.up4 = Up(128, 64)\n",
    "\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "\n",
    "        x2 = self.down1(x1)\n",
    "\n",
    "        x3 = self.down2(x2)\n",
    "\n",
    "        x4 = self.down3(x3)\n",
    "\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        x = self.up1(x5, x4)\n",
    "\n",
    "        x = self.up2(x, x3)\n",
    "\n",
    "        x = self.up3(x, x2)\n",
    "\n",
    "        x = self.up4(x, x1)\n",
    "\n",
    "        logits = self.outc(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = UNet(1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "# Loss = nn.CrossEntropyLoss()\n",
    "# Loss = DiceLoss()\n",
    "Loss = FocalLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50.13752293586731\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "241.38504600524902\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "482.30786299705505\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "952.3016798496246\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "1872.6846117973328\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "4999.378232002258\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "5942.359857797623\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "6886.096838951111\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "7856.647285938263\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "8868.682764053345\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "9823.463773965836\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "data_path = './data/train'\n",
    "# path = './data/unet_10.pt'\n",
    "num_workers = 4\n",
    "batch_size = 4\n",
    "train_set = TrainDataset(data_path)\n",
    "training_data_loader = DataLoader(dataset=train_set, num_workers=num_workers, batch_size=batch_size, shuffle=True)\n",
    "num_epochs = 160\n",
    "# Fetch images and labels. \n",
    "\n",
    "start = time.time()\n",
    "record = [0, 4, 9, 19, 39, 59, 79, 99, 119, 139, 159]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    model.train()\n",
    "    for iteration, sample in enumerate(training_data_loader):\n",
    "        img, mask, name = sample\n",
    "\n",
    "        img1 = img.unsqueeze(1)\n",
    "\n",
    "        # Write your FORWARD below\n",
    "        # Note: Input image to your model and ouput the predicted mask and Your predicted mask should have 4 channels\n",
    "    \n",
    "\n",
    " \n",
    "        y_predict = model.forward(img1)\n",
    "        \n",
    "\n",
    "#         mask = torch.tensor(mask, dtype = torch.long)\n",
    "        mask = mask.type(torch.long)\n",
    "\n",
    "        loss = Loss(y_predict, mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Then write your BACKWARD & OPTIMIZE below\n",
    "        # Note: Compute Loss and Optimize\n",
    "    \n",
    "       \n",
    "    if epoch in record: \n",
    "        torch.save(model.state_dict(), './data/unet_512_{}_focal.pt'.format(epoch))\n",
    "        finish = time.time()\n",
    "        train_time = finish - start\n",
    "        print(train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "val_path = './data/val'\n",
    "\n",
    "path = './data/unet_512_59_focal.pt'\n",
    "\n",
    "num_workers = 4\n",
    "batch_size = 2\n",
    "\n",
    "val_set = TrainDataset(val_path)\n",
    "val_data_loader = DataLoader(dataset=val_set, num_workers=num_workers,batch_size=batch_size, shuffle=False)\n",
    "\n",
    "i= 101\n",
    "\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()\n",
    "for iteration, sample in enumerate(val_data_loader):\n",
    "    \n",
    "    img, mask, name = sample\n",
    "    img2 = img.unsqueeze(1)\n",
    "\n",
    "    x = model.forward(img2)\n",
    "    \n",
    "    mask1 = x[0]\n",
    "    mask2 = x[1]\n",
    "\n",
    "    \n",
    "    mask1 = torch.argmax(mask1.squeeze(), dim=0)\n",
    "    mask2 = torch.argmax(mask2.squeeze(), dim=0)\n",
    "\n",
    "    mask1 = mask1.detach().numpy()\n",
    "    mask2 = mask2.detach().numpy()\n",
    "    cv2.imwrite('./data/val/mask2/cmr{}_mask.png'.format(i), mask1)\n",
    "    i += 1\n",
    "    cv2.imwrite('./data/val/mask2/cmr{}_mask.png'.format(i), mask2)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this block you are expected to write code to load saved model and deploy it to all data in test set to \n",
    "# produce segmentation masks in png images valued 0,1,2,3, which will be used for the submission to Kaggle.\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "test_path = './data/test'\n",
    "num_workers = 4\n",
    "batch_size = 2\n",
    "\n",
    "test_set = TestDataset(test_path)\n",
    "test_data_loader = DataLoader(dataset=test_set, num_workers=num_workers, batch_size = batch_size, shuffle=False)\n",
    "i= 121\n",
    "\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()\n",
    "for sample, name in test_data_loader:\n",
    "\n",
    "    img = sample.unsqueeze(1)\n",
    "\n",
    "    x = model.forward(img)\n",
    "    \n",
    "    mask1 = x[0]\n",
    "    mask2 = x[1]\n",
    "    \n",
    "    mask1 = torch.argmax(mask1.squeeze(), dim=0)\n",
    "    mask2 = torch.argmax(mask2.squeeze(), dim=0)\n",
    "\n",
    "\n",
    "    mask1 = mask1.detach().numpy()\n",
    "    mask2 = mask2.detach().numpy()\n",
    "\n",
    "    cv2.imwrite('./data/test/mask/cmr{}_mask.png'.format(i), mask1)\n",
    "    i += 1\n",
    "    cv2.imwrite('./data/test/mask/cmr{}_mask.png'.format(i), mask2)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_dice(mask1, mask2, label_class=1):\n",
    "    \"\"\"\n",
    "    Dice score of a specified class between two volumes of label masks.\n",
    "    (classes are encoded but by label class number not one-hot )\n",
    "    Note: stacks of 2D slices are considered volumes.\n",
    "\n",
    "    Args:\n",
    "        mask1: N label masks, numpy array shaped (H, W, N)\n",
    "        mask2: N label masks, numpy array shaped (H, W, N)\n",
    "        label_class: the class over which to calculate dice scores\n",
    "\n",
    "    Returns:\n",
    "        volume_dice\n",
    "    \"\"\"\n",
    "    mask1_pos = (mask1 == label_class).astype(np.float32)\n",
    "    mask2_pos = (mask2 == label_class).astype(np.float32)\n",
    "    dice = 2 * np.sum(mask1_pos * mask2_pos) / (np.sum(mask1_pos) + np.sum(mask2_pos))\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7509597225503825\n",
      "0.6461474156800421\n",
      "0.9252988901152079\n",
      "0.925439738122796\n",
      "0.9284351278260599\n",
      "0.8352561788588977\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/val'\n",
    "mask1 =cv2.imread(os.path.join(data_dir,'mask','cmr113_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "mask2 = cv2.imread(os.path.join(data_dir,'mask2','cmr113_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "label_class = 2\n",
    "x = categorical_dice(mask1, mask2, label_class)\n",
    "mask11 =cv2.imread(os.path.join(data_dir,'mask','cmr111_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "mask12 = cv2.imread(os.path.join(data_dir,'mask2','cmr111_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "mask21 =cv2.imread(os.path.join(data_dir,'mask','cmr112_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "mask22 = cv2.imread(os.path.join(data_dir,'mask2','cmr112_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "mask31 =cv2.imread(os.path.join(data_dir,'mask','cmr113_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "mask32 = cv2.imread(os.path.join(data_dir,'mask2','cmr113_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "mask41 =cv2.imread(os.path.join(data_dir,'mask','cmr114_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "mask42 = cv2.imread(os.path.join(data_dir,'mask2','cmr114_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "mask51 =cv2.imread(os.path.join(data_dir,'mask','cmr115_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "mask52 = cv2.imread(os.path.join(data_dir,'mask2','cmr115_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "\n",
    "true_mask =np.array([mask11, mask21, mask31, mask41, mask51])\n",
    "predict_mask = np.array([mask12, mask22, mask32, mask42, mask52])\n",
    "\n",
    "\n",
    "# label_class = 2\n",
    "# x = categorical_dice(mask1, mask2)\n",
    "# print(x)\n",
    "sum5 = 0\n",
    "for i in range(5):\n",
    "\n",
    "    summary = 0\n",
    "    labe_class = 1\n",
    "    for j in range(3):\n",
    "        x = categorical_dice(true_mask[i], predict_mask[i], labe_class)\n",
    "        summary +=x\n",
    "        labe_class +=1\n",
    "    average = summary/3\n",
    "    sum5 +=average\n",
    "    print(average)\n",
    "\n",
    "print(sum5/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def rle_encoding(x):\n",
    "    '''\n",
    "    *** Credit to https://www.kaggle.com/rakhlin/fast-run-length-encoding-python ***\n",
    "    x: numpy array of shape (height, width), 1 - mask, 0 - background\n",
    "    Returns run length as list\n",
    "    '''\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b > prev + 1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "\n",
    "def submission_converter(mask_directory, path_to_save):\n",
    "    writer = open(os.path.join(path_to_save, \"submission.csv\"), 'w')\n",
    "    writer.write('id,encoding\\n')\n",
    "\n",
    "    files = os.listdir(mask_directory)\n",
    "\n",
    "\n",
    "\n",
    "    for file in files:\n",
    "        name = file[:-4]\n",
    "        mask = cv2.imread(os.path.join(mask_directory, file), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        \n",
    "        mask1 = (mask == 1)\n",
    "        mask2 = (mask == 2)\n",
    "        mask3 = (mask == 3)\n",
    "\n",
    "\n",
    "        encoded_mask1 = rle_encoding(mask1)\n",
    "        encoded_mask1 = ' '.join(str(e) for e in encoded_mask1)\n",
    "        encoded_mask2 = rle_encoding(mask2)\n",
    "        encoded_mask2 = ' '.join(str(e) for e in encoded_mask2)\n",
    "        encoded_mask3 = rle_encoding(mask3)\n",
    "        encoded_mask3 = ' '.join(str(e) for e in encoded_mask3)\n",
    "\n",
    "        writer.write(name + '1,' + encoded_mask1 + \"\\n\")\n",
    "        writer.write(name + '2,' + encoded_mask2 + \"\\n\")\n",
    "        writer.write(name + '3,' + encoded_mask3 + \"\\n\")\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_converter(\"/Users/nolan/CW2/data/test/mask\", \"/Users/nolan/CW2/data/test/submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
